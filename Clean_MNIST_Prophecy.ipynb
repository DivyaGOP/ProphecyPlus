{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clean MNIST Prophecy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safednn-nasa/Prophecy/blob/master/Clean_MNIST_Prophecy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-JL_MKop5Qh",
        "colab_type": "code",
        "outputId": "2247e927-4d13-4561-a0a7-d7f961773fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "import math\n",
        "import io\n",
        "import os\n",
        "from collections import namedtuple\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn import tree\n",
        "from tqdm import tqdm\n",
        "import operator\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RelAEm1T3JVq",
        "colab_type": "code",
        "outputId": "0c2c7278-4708-4669-ebe8-168a3d1b9525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#!python \n",
        "!pip3 install -U pybind11"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybind11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/4d/ae1c4d8e8b139afa9682054dd42df3b0e3b5c1731287933021b9fd7e9cc4/pybind11-2.4.3-py2.py3-none-any.whl (150kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 27.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 81kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 92kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 102kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 112kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 122kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 133kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 143kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 5.1MB/s \n",
            "\u001b[?25hInstalling collected packages: pybind11\n",
            "Successfully installed pybind11-2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns8_eYDFReWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 50\n",
        "TEST_BATCH_SIZE = 1000  \n",
        "IMAGE_SIZE = 28\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "LAYER = 7\n",
        "LABEL = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO1Z5bHe06kz",
        "colab_type": "text"
      },
      "source": [
        "## **Load Input Data For MNIST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3Zs1CzKSqjM",
        "colab_type": "code",
        "outputId": "a13e968e-0c8b-42a6-9046-37cc7b77abe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "mnist = input_data.read_data_sets('MNIST_data')\n",
        "\n",
        "\n",
        "def read_inputs_from_file(inputFile, height, width):\n",
        "    global inputMatrix, labelMatrix, mnist_inp_images, mnist_inp_labels\n",
        "    with open(inputFile) as f:\n",
        "        lines = f.readlines()\n",
        "        print(len(lines), \"examples\")\n",
        "        inputMatrix = np.empty(len(lines),dtype=list)\n",
        "        labelMatrix = np.zeros(len(lines),dtype=int)\n",
        "        mnist_inp_images = np.zeros([8,784],dtype=float)\n",
        "        mnist_inp_labels = np.zeros(8,dtype=int)\n",
        "        for l in range(len(lines)):\n",
        "            if (l == 8):\n",
        "              break\n",
        "            k = [float(stringIn) for stringIn in lines[l].split(',')[1:]] #This is to remove the useless 1 at the start of each string. Not sure why that's there.\n",
        "            inputMatrix[l] = np.zeros((height, width, 1),dtype=float) #we're asuming that everything is 2D for now. The 1 is just to keep numpy happy.\n",
        "            labelMatrix[l] = lines[l].split(',')[0]\n",
        "            mnist_inp_labels[l] = labelMatrix[l]\n",
        "            count = 0\n",
        "            for i in range(height):\n",
        "                for j in range(width):\n",
        "                        print(count)\n",
        "                        inputMatrix[l][i][j] = k[count]\n",
        "                        print(k[count])\n",
        "                        pixel_num = (i *28) + j\n",
        "                        mnist_inp_images[l][pixel_num] = inputMatrix[l][i][j]\n",
        "                        count += 1\n",
        " \n",
        "#!wget https://raw.githubusercontent.com/safednn-nasa/prophecy_DNN/sym_convnn/master/MNIST_ITR_REL/mnist_inputs1.txt -O ./mnist_train_labels.txt\n",
        "\n",
        "#read_inputs_from_file('./mnist_train_labels.txt', 28, 28)\n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C45xGa98mzt",
        "colab_type": "text"
      },
      "source": [
        "##**Architecture Of the MNIST Model (10 Layers) In TENSORFLOW** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39JpLmTitqdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def weight_variable(shape, name):\n",
        "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "  return tf.Variable(initial, name=name)\n",
        "\n",
        "def bias_variable(shape, name):\n",
        "  initial = tf.constant(0.1, shape=shape)\n",
        "  return tf.Variable(initial, name=name)\n",
        "\n",
        "def fc2d(x, W):\n",
        "  return tf.nn.fc2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
        "                        strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "def create_model():\n",
        "    x = tf.identity(tf.placeholder(tf.float32, shape=[None, 784]), name=\"input\")\n",
        "    y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "\n",
        "    W_fc1 = weight_variable([784, 10],name='w_fc1')\n",
        "    b_fc1 = bias_variable([10],name='b_fc1')\n",
        "    x_image = tf.reshape(x, [-1, 784])\n",
        "    h_fc1_relu_inp = tf.identity(tf.matmul(x_image, W_fc1) + b_fc1, name='h_fc1_relu_inp')\n",
        "    h_fc1 = tf.nn.relu(tf.matmul(x_image, W_fc1) + b_fc1)\n",
        "#    h_fc1_iden = tf.identity(h_fc1,name='h_fc1')\n",
        "    \n",
        "    \n",
        "    W_fc2 = weight_variable([10, 10],name='w_fc2')\n",
        "    b_fc2 = bias_variable([10],name='b_fc2')\n",
        "    h_fc2_relu_inp = tf.identity(tf.matmul(h_fc1, W_fc2) + b_fc2, name='h_fc2_relu_inp')\n",
        "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
        " #   h_fc2_iden = tf.identity(h_fc2,name='h_fc2')\n",
        "    \n",
        "    \n",
        "    W_fc3 = weight_variable([10, 10],name='w_fc3')\n",
        "    b_fc3 = bias_variable([10],name='b_fc3')\n",
        "    h_fc3_relu_inp = tf.identity(tf.matmul(h_fc2, W_fc3) + b_fc3, name='h_fc3_relu_inp')\n",
        "    h_fc3 = tf.nn.relu(tf.matmul(h_fc2, W_fc3) + b_fc3)\n",
        "  #  h_fc3_iden = tf.identity(h_fc3,name='h_fc3')\n",
        "\n",
        "    W_fc4 = weight_variable([10, 10],name='w_fc4')\n",
        "    b_fc4 = bias_variable([10],name='b_fc4')\n",
        "    h_fc4_relu_inp = tf.identity(tf.matmul(h_fc3, W_fc4) + b_fc4, name='h_fc4_relu_inp')\n",
        "    h_fc4 = tf.nn.relu(tf.matmul(h_fc3, W_fc4) + b_fc4)\n",
        "   # h_fc4_iden = tf.identity(h_fc4,name='h_fc4')\n",
        "    \n",
        "    \n",
        "    W_fc5 = weight_variable([10, 10],name='w_fc5')\n",
        "    b_fc5 = bias_variable([10],name='b_fc5')\n",
        "    h_fc5_relu_inp = tf.identity(tf.matmul(h_fc4, W_fc5) + b_fc5, name='h_fc5_relu_inp')\n",
        "    h_fc5 = tf.nn.relu(tf.matmul(h_fc4, W_fc5) + b_fc5)\n",
        "    #h_fc5_iden = tf.identity(h_fc5,name='h_fc5')\n",
        "\n",
        "   \n",
        "    W_fc6 = weight_variable([10, 10],name='w_fc6')\n",
        "    b_fc6 = bias_variable([10],name='b_fc6')\n",
        "    h_fc6_relu_inp = tf.identity(tf.matmul(h_fc5, W_fc6) + b_fc6, name='h_fc6_relu_inp')\n",
        "    h_fc6 = tf.nn.relu(tf.matmul(h_fc5, W_fc6) + b_fc6)\n",
        "   # h_fc6_iden = tf.identity(h_fc6,name='h_fc6')\n",
        "    \n",
        "    \n",
        "    W_fc7 = weight_variable([10, 10],name='w_fc7')\n",
        "    b_fc7 = bias_variable([10],name='b_fc7')\n",
        "    h_fc7_relu_inp = tf.identity(tf.matmul(h_fc6, W_fc7) + b_fc7, name='h_fc7_relu_inp')\n",
        "    h_fc7 = tf.nn.relu(tf.matmul(h_fc6, W_fc7) + b_fc7)\n",
        "   # h_fc7_iden = tf.identity(h_fc7,name='h_fc7')\n",
        "\n",
        "    W_fc8 = weight_variable([10, 10],name='w_fc8')\n",
        "    b_fc8 = bias_variable([10],name='b_fc8')\n",
        "    h_fc8_relu_inp = tf.identity(tf.matmul(h_fc7, W_fc8) + b_fc8, name='h_fc8_relu_inp')\n",
        "    h_fc8 = tf.nn.relu(tf.matmul(h_fc7, W_fc8) + b_fc8)\n",
        "   # h_fc8_iden = tf.identity(h_fc8,name='h_fc8')\n",
        "    \n",
        "    \n",
        "    W_fc9 = weight_variable([10, 10],name='w_fc9')\n",
        "    b_fc9 = bias_variable([10],name='b_fc9')\n",
        "    h_fc9_relu_inp = tf.identity(tf.matmul(h_fc8, W_fc9) + b_fc9, name='h_fc9_relu_inp')\n",
        "    h_fc9 = tf.nn.relu(tf.matmul(h_fc8, W_fc9) + b_fc9)\n",
        "   # h_fc9_iden = tf.identity(h_fc9,name='h_fc9')\n",
        "\n",
        "    W_fc10 = weight_variable([10, 10],name='w_fc10')\n",
        "    b_fc10 = bias_variable([10],name='b_fc10')\n",
        "    h_fc10_relu_inp = tf.identity(tf.matmul(h_fc9, W_fc10) + b_fc10, name='h_fc10_relu_inp')\n",
        "    h_fc10 = tf.nn.relu(tf.matmul(h_fc9, W_fc10) + b_fc10)\n",
        "    #h_fc10_iden = tf.identity(h_fc10,name='h_fc10')\n",
        "\n",
        "    W_fc11 = weight_variable([10, 10],name='w_fc11')\n",
        "    b_fc11 = bias_variable([10],name='b_fc11')\n",
        "    \n",
        "    y_fc = tf.matmul(h_fc10, W_fc11) + b_fc11\n",
        "    h_y_fc_iden = tf.identity(y_fc,name='y_fc')\n",
        "    \n",
        "    \n",
        "    prediction = tf.identity(tf.nn.softmax(y_fc), name=\"import/prediction\")\n",
        "    \n",
        "    \n",
        "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_fc))\n",
        "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "    correct_prediction = tf.equal(tf.argmax(y_fc, 1), tf.argmax(y_, 1))\n",
        "    \n",
        "    \n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    \n",
        "\n",
        "    return cross_entropy, accuracy, x, keep_prob, y_fc, y_, W_fc1, W_fc2, W_fc3, W_fc4, W_fc5, W_fc6, W_fc7, W_fc8, W_fc9, W_fc10, W_fc11, b_fc1, b_fc2, b_fc3, b_fc4, b_fc5, b_fc6, b_fc7, b_fc8, b_fc9, b_fc10, b_fc11, h_fc1, h_fc1_relu_inp, h_fc2, h_fc2_relu_inp, h_fc3, h_fc3_relu_inp, h_fc4, h_fc4_relu_inp, h_fc5, h_fc5_relu_inp, h_fc6, h_fc6_relu_inp, h_fc7, h_fc7_relu_inp, h_fc8, h_fc8_relu_inp, h_fc9, h_fc9_relu_inp, h_fc10, h_fc10_relu_inp, y_fc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv3U6Xs_T2el",
        "colab_type": "text"
      },
      "source": [
        "### **Train a new model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-2x6UTcuJUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.reset_default_graph()\n",
        "#sess = tf.InteractiveSession()\n",
        "#cross_entropy, accuracy, x, keep_prob, y_conv, y_ = create_model()\n",
        "#train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "#saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n",
        "#sess.run(tf.global_variables_initializer())\n",
        "#for i in range(0, 1200):\n",
        "#  batch = mnist.train.next_batch(BATCH_SIZE)\n",
        "#  train_step.run(feed_dict={x: batch[0], y_: np.eye(10)[batch[1]], keep_prob: 0.5})\n",
        "#  if i%100 == 0:\n",
        "#    test_accuracy = accuracy.eval(feed_dict={\n",
        "#        x:mnist.test.images, y_: np.eye(10)[mnist.test.labels], keep_prob: 1.0})\n",
        "#    print(\"step %d, test accuracy %g\"%(i, test_accuracy))    \n",
        "#ckpt_path_name = saver.save(sess, './checkpoints/mnist_invariant.ckpt', global_step=i)\n",
        "#print \"Checkpoint saved at: %s\" % ckpt_path_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylVCQ9lfTfWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import files\n",
        "#files.download(ckpt_path_name + '.data-00000-of-00001')\n",
        "#files.download(ckpt_path_name + '.index')\n",
        "#files.download(ckpt_path_name + '.meta')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy6_gvmpT4Wv",
        "colab_type": "text"
      },
      "source": [
        "### Restore a pretrained model from check point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMibD_lwWdVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!mkdir -p ./checkpoints\n",
        "#!wget https://github.com/safednn-nasa/prophecy_DNN/sym_convnn/raw/master/MNIST_ITR_REL/MNIST_conv_checkpoint/mnist_invariants.ckpt.index -O ./checkpoints/mnist_invariants.ckpt.index\n",
        "#!wget https://github.com/safednn-nasa/prophecy_DNN/sym_convnn/raw/master/MNIST_ITR_REL/MNIST_conv_checkpoint/mnist_invariants.ckpt.meta -O ./checkpoints/mnist_invariants.ckpt.meta\n",
        "#!wget https://github.com/safednn-nasa/prophecy_DNN/sym_convnn/raw/master/MNIST_ITR_REL/MNIST_conv_checkpoint/mnist_invariants.ckpt.data-00000-of-00001 -O ./checkpoints/mnist_invariants.ckpt.data-00000-of-00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwDRpu0_1QDv",
        "colab_type": "text"
      },
      "source": [
        "##**Restore pre-trained model from a .nn file with weights and biases**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JmFzaPPT6ql",
        "colab_type": "code",
        "outputId": "c7211dbf-3b7d-43a2-dc1d-cdec0a83934e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "def read_weights_from_file(inputFile):\n",
        "    global weightMatrix, biasMatrix\n",
        "    \n",
        "    with open(inputFile) as f:\n",
        "        lines = f.readlines()\n",
        "        for indx in range(0,len(lines)):\n",
        "            #print(indx, lines[indx])\n",
        "            numberOfLayers = int(lines[0])\n",
        "            numberOfLayers = 11\n",
        "            weightMatrix = np.empty(numberOfLayers, dtype=list)\n",
        "            biasMatrix = np.empty(numberOfLayers, dtype=list)\n",
        "            currentLine = 2\n",
        "            for i in range(numberOfLayers):\n",
        "              dimensions = lines[currentLine].split(',')\n",
        "              dimensions = [int(stringDimension) for stringDimension in dimensions]\n",
        "              #print dimensions\n",
        "              currentLine += 1\n",
        "              weights = [float(stringWeight) for stringWeight in lines[currentLine].split(',')]\n",
        "              #print len(weights)\n",
        "              count = 0\n",
        "              weightMatrix[i] = np.zeros((dimensions[0], dimensions[1]), dtype=float)\n",
        "              for j in range(dimensions[1]):\n",
        "                 for k in range(dimensions[0]):\n",
        "                      weightMatrix[i][k][j] = weights[count]\n",
        "                      count += 1\n",
        "              currentLine += 1\n",
        "              biases = [float(stringBias) for stringBias in lines[currentLine].split(',')]\n",
        "              biasMatrix[i] = np.zeros(len(biases))\n",
        "              biasMatrix[i] = biases\n",
        "              currentLine += 2\n",
        "\n",
        "!wget https://raw.githubusercontent.com/safednn-nasa/prophecy_DNN/master/MNIST_ITR_REL/mnist_10_layer.txt -O ./mnist_10_layer.txt\n",
        "\n",
        "\n",
        "read_weights_from_file('./mnist_10_layer.txt')\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "cross_entropy, accuracy, x, keep_prob, y_fc, y_, W_fc1, W_fc2, W_fc3, W_fc4, W_fc5, W_fc6, W_fc7, W_fc8, W_fc9, W_fc10, W_fc11,b_fc1, b_fc2, b_fc3, b_fc4, b_fc5, b_fc6, b_fc7, b_fc8, b_fc9, b_fc10, b_fc11, h_fc1, h_fc1_relu_inp, h_fc2, h_fc2_relu_inp, h_fc3, h_fc3_relu_inp,h_fc4, h_fc4_relu_inp, h_fc5, h_fc5_relu_inp, h_fc6, h_fc6_relu_inp, h_fc7, h_fc7_relu_inp, h_fc8, h_fc8_relu_inp, h_fc9, h_fc9_relu_inp, h_fc10, h_fc10_relu_inp, y_fc = create_model()\n",
        "feed_dict = {x:mnist.test.images, y_: np.eye(10)[mnist.test.labels], keep_prob: 1.0, W_fc1: weightMatrix[0], b_fc1: biasMatrix[0], W_fc2: weightMatrix[1], b_fc2: biasMatrix[1], W_fc3: weightMatrix[2], b_fc3: biasMatrix[2], W_fc4: weightMatrix[3], b_fc4: biasMatrix[3], W_fc5: weightMatrix[4], b_fc5: biasMatrix[4], W_fc6: weightMatrix[5], b_fc6: biasMatrix[5],W_fc7: weightMatrix[6], b_fc7: biasMatrix[6],W_fc8: weightMatrix[7], b_fc8: biasMatrix[7],W_fc9: weightMatrix[8], b_fc9: biasMatrix[8], W_fc10: weightMatrix[9], b_fc10: biasMatrix[9], W_fc11: weightMatrix[10], b_fc11: biasMatrix[10]}\n",
        "  \n",
        "test_accuracy = accuracy.eval(feed_dict)\n",
        "print(\"Test accuracy %g\"%(test_accuracy))   \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-07 08:33:42--  https://raw.githubusercontent.com/safednn-nasa/prophecy_DNN/master/MNIST_ITR_REL/mnist_10_layer.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 146765 (143K) [text/plain]\n",
            "Saving to: ‘./mnist_10_layer.txt’\n",
            "\n",
            "\r./mnist_10_layer.tx   0%[                    ]       0  --.-KB/s               \r./mnist_10_layer.tx 100%[===================>] 143.33K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-12-07 08:33:42 (6.56 MB/s) - ‘./mnist_10_layer.txt’ saved [146765/146765]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy 0.933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR9bYFN9E9lg",
        "colab_type": "text"
      },
      "source": [
        "## Get tensors from pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2F_QleSkgvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cross_entropy, accuracy, x, keep_prob, y_fc, y_, W_fc1, W_fc2, W_fc3, W_fc4, W_fc5, W_fc6, W_fc7, W_fc8, W_fc9, W_fc10, W_fc11,b_fc1, b_fc2, b_fc3, b_fc4, b_fc5, b_fc6, b_fc7, b_fc8, b_fc9, b_fc10, b_fc11, h_fc1, h_fc2, h_fc2_relu_inp, h_fc3, h_fc3_relu_inp,h_fc4, h_fc4_relu_inp, h_fc5, h_fc5_relu_inp, h_fc6, h_fc7, h_fc8, h_fc9, h_fc10, y_fc = create_model()\n",
        "\n",
        "cross_entropy, accuracy, x, keep_prob, y_fc, y_, W_fc1, W_fc2, W_fc3, W_fc4, W_fc5, W_fc6, W_fc7, W_fc8, W_fc9, W_fc10, W_fc11,b_fc1, b_fc2, b_fc3, b_fc4, b_fc5, b_fc6, b_fc7, b_fc8, b_fc9, b_fc10, b_fc11, h_fc1, h_fc1_relu_inp, h_fc2, h_fc2_relu_inp, h_fc3, h_fc3_relu_inp,h_fc4, h_fc4_relu_inp, h_fc5, h_fc5_relu_inp, h_fc6, h_fc6_relu_inp, h_fc7, h_fc7_relu_inp, h_fc8, h_fc8_relu_inp, h_fc9, h_fc9_relu_inp, h_fc10, h_fc10_relu_inp, y_fc = create_model()\n",
        "\n",
        "t_fc1 = h_fc1 #sess.graph.get_tensor_by_name('h_fc1:0')\n",
        "t_fc2 = h_fc2 #sess.graph.get_tensor_by_name('h_fc2:0')\n",
        "t_fc3 = h_fc3 #sess.graph.get_tensor_by_name('h_fc3:0')\n",
        "t_fc4 = h_fc4 #sess.graph.get_tensor_by_name('h_fc4:0')\n",
        "t_fc5 = h_fc5 #sess.graph.get_tensor_by_name('h_fc5:0')\n",
        "t_fc6 = h_fc6 #sess.graph.get_tensor_by_name('h_fc6:0')\n",
        "t_fc7 = h_fc7 #sess.graph.get_tensor_by_name('h_fc7:0')\n",
        "t_fc8 = h_fc8 #sess.graph.get_tensor_by_name('h_fc8:0')\n",
        "t_fc9 = h_fc9 #sess.graph.get_tensor_by_name('h_fc9:0')\n",
        "t_fc10 = h_fc10 #sess.graph.get_tensor_by_name('h_fc10:0')\n",
        "\n",
        "if (LAYER == 1):\n",
        "  curr_lay = t_fc1\n",
        "  prev_lay = t_fc1\n",
        "  curr_hlay = h_fc1\n",
        "  prev_hlay = h_fc1\n",
        "  inp_lay = h_fc1_relu_inp\n",
        "  \n",
        "if (LAYER == 2):\n",
        "  curr_lay = t_fc2\n",
        "  prev_lay = t_fc1\n",
        "  curr_hlay = h_fc2\n",
        "  prev_hlay = h_fc1\n",
        "  inp_lay = h_fc2_relu_inp\n",
        "  \n",
        "if (LAYER == 3):\n",
        "  curr_lay = t_fc3\n",
        "  prev_lay = t_fc2\n",
        "  curr_hlay = h_fc3\n",
        "  prev_hlay = h_fc2\n",
        "  inp_lay = h_fc3_relu_inp\n",
        "  \n",
        "if (LAYER == 4):\n",
        "  curr_lay = t_fc4\n",
        "  prev_lay = t_fc3\n",
        "  curr_hlay = h_fc4\n",
        "  prev_hlay = h_fc3\n",
        "  inp_lay = h_fc4_relu_inp\n",
        "\n",
        "if (LAYER == 5):\n",
        "  curr_lay = t_fc5\n",
        "  prev_lay = t_fc4\n",
        "  curr_hlay = h_fc5\n",
        "  prev_hlay = h_fc4\n",
        "  inp_lay = h_fc5_relu_inp\n",
        "  \n",
        "if (LAYER == 6):\n",
        "  curr_lay = t_fc6\n",
        "  prev_lay = t_fc5\n",
        "  curr_hlay = h_fc6\n",
        "  prev_hlay = h_fc5\n",
        "  inp_lay = h_fc6_relu_inp\n",
        "  \n",
        "if (LAYER == 7):\n",
        "  curr_lay = t_fc7\n",
        "  prev_lay = t_fc6\n",
        "  curr_hlay = h_fc7\n",
        "  prev_hlay = h_fc6\n",
        "  inp_lay = h_fc7_relu_inp\n",
        "  \n",
        "if (LAYER == 8):\n",
        "  curr_lay = t_fc8\n",
        "  prev_lay = t_fc7\n",
        "  curr_hlay = h_fc8\n",
        "  prev_hlay = h_fc7\n",
        "  inp_lay = h_fc8_relu_inp\n",
        "  \n",
        "if (LAYER == 9):\n",
        "  curr_lay = t_fc9\n",
        "  prev_lay = t_fc8\n",
        "  curr_hlay = h_fc9\n",
        "  prev_hlay = h_fc8\n",
        "  inp_lay = h_fc9_relu_inp\n",
        "  \n",
        "if (LAYER == 10):\n",
        "  curr_lay = t_fc10\n",
        "  prev_lay = t_fc9\n",
        "  curr_hlay = h_fc10\n",
        "  prev_hlay = h_fc9\n",
        "  inp_lay = h_fc10_relu_inp\n",
        "  \n",
        "t_label = tf.placeholder(tf.int32)\n",
        "t_neuron_id = tf.placeholder(tf.int32)\n",
        "t_grad = tf.gradients(y_fc[:, t_label], x)\n",
        "t_grad_neuron = tf.gradients(y_fc[:, t_label], curr_hlay)[0]\n",
        "t_grad_conductance = tf.gradients(inp_lay[:,t_neuron_id], x, grad_ys=t_grad_neuron[:, t_neuron_id])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL92gsWskV0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "\n",
        "def get_prediction(inps, tensor=y_fc, batch_size=100):\n",
        "  def get_prediction_batch(batch):\n",
        "    #feed = {x: np.array(batch), keep_prob:1.0}\n",
        "    feed = {x: np.array(batch), keep_prob:1.0, W_fc1: weightMatrix[0], b_fc1: biasMatrix[0], W_fc2: weightMatrix[1], b_fc2: biasMatrix[1], W_fc3: weightMatrix[2], b_fc3: biasMatrix[2], W_fc4: weightMatrix[3], b_fc4: biasMatrix[3], W_fc5: weightMatrix[4], b_fc5: biasMatrix[4], W_fc6: weightMatrix[5], b_fc6: biasMatrix[5],W_fc7: weightMatrix[6], b_fc7: biasMatrix[6],W_fc8: weightMatrix[7], b_fc8: biasMatrix[7],W_fc9: weightMatrix[8], b_fc9: biasMatrix[8], W_fc10: weightMatrix[9], b_fc10: biasMatrix[9], W_fc11: weightMatrix[10], b_fc11: biasMatrix[10]}\n",
        "    return sess.run(tensor, feed_dict=feed)\n",
        "  n = len(inps)\n",
        "  if n%batch_size == 0:\n",
        "    batches = [inps[i*batch_size:(i+1)*batch_size] for i in range(int(n/batch_size))]\n",
        "  else:\n",
        "    batches = [inps[i*batch_size:(i+1)*batch_size] for i in range(int(n/batch_size) +1)]    \n",
        "  batch_predictions = [get_prediction_batch(b) for b in tqdm(batches)]\n",
        "  return np.concatenate(tuple(batch_predictions), axis=0)\n",
        "\n",
        "def attribute(inp, label, baseline=None, steps=50, use_top_label=False):\n",
        "  def top_label(inp):\n",
        "    return np.argmax(get_prediction([inp])[0])\n",
        "  if baseline is None:\n",
        "    baseline = 0*inp\n",
        "  scaled_inputs = [baseline + (float(i)/steps)*(inp-baseline) for i in range(0, steps)]\n",
        "  #feed = {keep_prob:1.0}\n",
        "  feed = {keep_prob:1.0, W_fc1: weightMatrix[0], b_fc1: biasMatrix[0], W_fc2: weightMatrix[1], b_fc2: biasMatrix[1], W_fc3: weightMatrix[2], b_fc3: biasMatrix[2], W_fc4: weightMatrix[3], b_fc4: biasMatrix[3], W_fc5: weightMatrix[4], b_fc5: biasMatrix[4], W_fc6: weightMatrix[5], b_fc6: biasMatrix[5],W_fc7: weightMatrix[6], b_fc7: biasMatrix[6],W_fc8: weightMatrix[7], b_fc8: biasMatrix[7],W_fc9: weightMatrix[8], b_fc9: biasMatrix[8], W_fc10: weightMatrix[9], b_fc10: biasMatrix[9], W_fc11: weightMatrix[10], b_fc11: biasMatrix[10]}\n",
        "    \n",
        "  if use_top_label:\n",
        "    feed[x] = [inp]\n",
        "    logits = sess.run(y_fc, feed_dict=feed)[0]\n",
        "    label = np.argmax(logits)\n",
        "  feed[x] = scaled_inputs\n",
        "  feed[t_label] = label\n",
        "  grads, scores = sess.run([t_grad, y_fc], feed_dict=feed)  # shapes: <steps+1>, <steps+1, inp.shape>\n",
        "  integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "#  print \"FINAL SCORE\", scores[-1][label]\n",
        "#  print \"BASELINE SCORE\", scores[0][label]\n",
        "#  print \"SUM\", np.sum(integrated_gradients), \"DIFF\", scores[-1][label] - scores[0][label]\n",
        "  return integrated_gradients\n",
        "\n",
        "def conductance(inp, label, neuron_id=None, baseline=None, steps=50):\n",
        "  # neuron_id is the id of the neuron in layer t_fc1 through which conductance\n",
        "  # must be computed. If None, vanilla IG is computed.\n",
        "  if baseline is None:\n",
        "    baseline = 0*inp\n",
        "  scaled_inputs = [baseline + (float(i)/steps)*(inp-baseline) for i in range(0, steps)]\n",
        "  feed = {keep_prob:1.0, W_fc1: weightMatrix[0], b_fc1: biasMatrix[0], W_fc2: weightMatrix[1], b_fc2: biasMatrix[1], W_fc3: weightMatrix[2], b_fc3: biasMatrix[2], W_fc4: weightMatrix[3], b_fc4: biasMatrix[3], W_fc5: weightMatrix[4], b_fc5: biasMatrix[4], W_fc6: weightMatrix[5], b_fc6: biasMatrix[5],W_fc7: weightMatrix[6], b_fc7: biasMatrix[6],W_fc8: weightMatrix[7], b_fc8: biasMatrix[7],W_fc9: weightMatrix[8], b_fc9: biasMatrix[8], W_fc10: weightMatrix[9], b_fc10: biasMatrix[9], W_fc11: weightMatrix[10], b_fc11: biasMatrix[10]}\n",
        "  feed[x] = scaled_inputs\n",
        "  feed[t_label] = label\n",
        "  if neuron_id != None:\n",
        "    feed[t_neuron_id] = neuron_id\n",
        "    grads, scores = sess.run([t_grad_conductance, y_fc], feed_dict=feed)  # shapes: <steps+1>, <steps+1, inp.shape>\n",
        "    integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "    return integrated_gradients\n",
        "  grads, scores = sess.run([t_grad, y_fc], feed_dict=feed)  # shapes: <steps+1>, <steps+1, inp.shape>    \n",
        "  integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "  #print \"FINAL SCORE\", scores[-1][label]\n",
        "  #print \"BASELINE SCORE\", scores[0][label]\n",
        "  #print \"SUM\", np.sum(integrated_gradients), \"DIFF\", scores[-1][label] - scores[0][label]\n",
        "  return integrated_gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_uhUyYiBlL7",
        "colab_type": "text"
      },
      "source": [
        "## Extracting Invariant Candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzZVWWP5O8qV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPvUSoGyFkdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fingerprint_suffix(inps):\n",
        "  # Below t_fc1 is the final fully connected layer of size 1024.\n",
        "  return (get_prediction(inps, tensor=t_fc6)>0.0).astype('int')\n",
        "\n",
        "def fingerprint_signature(inps,ten = t_fc1):\n",
        "  # Below t_fc1 is the final fully connected layer of size 1024.\n",
        "  return (get_prediction(inps, tensor=ten)>0.0).astype('int')\n",
        "\n",
        "def fingerprint_prefix(inps):\n",
        "  return (get_prediction(inps, tensor=tf.reshape(t_fc1, [-1, 14*14*32]))>0.0).astype('int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QarR8VNHZNk",
        "colab_type": "code",
        "outputId": "bd23a749-a0d6-4d81-bdba-dffb3c38b263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# train_suffixes, train_predictions are in the same order\n",
        "# as mnist.train.images. Henceforth when we use the index i we will\n",
        "# be referring to mnist.train.images[i].\n",
        "train_suffixes = fingerprint_signature(mnist.train.images, curr_lay)\n",
        "print(\"Suffixes computed for all training data\")\n",
        "train_predictions = np.argmax(get_prediction(mnist.train.images), axis=1)\n",
        "print(\"Predictions computed for all training data\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 550/550 [00:00<00:00, 643.40it/s]\n",
            " 11%|█         | 59/550 [00:00<00:00, 583.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Suffixes computed for all training data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 550/550 [00:00<00:00, 635.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predictions computed for all training data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxgnKvn04Q92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def describe_input(i, training=True):\n",
        "  print(\"Input\", i)\n",
        "  print(\"Groundtruth\", mnist.train.labels[i])\n",
        "  print(\"Prediction\", train_predictions[i])\n",
        "  print(\"Fine-grained prediction\", 10*mnist.train.labels[i] + train_predictions[i])\n",
        "  show_mnist_img(mnist.train.images[i])\n",
        "  \n",
        "def describe_input_INP(i):\n",
        "  print(\"Input\", i)\n",
        "  print(\"Groundtruth\", mnist_inp_labels[i])\n",
        "  print(\"Prediction\", inp_predictions[i])\n",
        "  show_mnist_img(mnist_inp_images[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fcVM2dek4Qo",
        "colab_type": "text"
      },
      "source": [
        "### Build the Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_0fvX3aAWHN",
        "colab_type": "code",
        "outputId": "64f183b4-d638-4f97-a9f8-aa7fa5606f97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Basic decision tree\n",
        "basic_estimator = tree.DecisionTreeClassifier()\n",
        "basic_estimator.fit(train_suffixes, train_predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "                       max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort=False,\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkvj66ZvX3B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fine-grained predictions decision tree\n",
        "#fine_grained_predictions = 10*mnist.train.labels + train_predictions\n",
        "#fine_grained_estimator = tree.DecisionTreeClassifier()\n",
        "#fine_grained_estimator.fit(train_suffixes, fine_grained_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfwDuDQtprdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decision tree per label\n",
        "def get_relative_predictions(label):\n",
        "  #print \"Create relative predictions for label:%d\" % label\n",
        "  res = np.zeros(train_predictions.shape)\n",
        "  for i in range(len(train_predictions)):\n",
        "    pred = train_predictions[i]\n",
        "    gt = mnist.train.labels[i]\n",
        "    if gt == label and pred == gt:\n",
        "      res[i] = 0\n",
        "    elif gt == label and pred != gt:\n",
        "      res[i] = 1\n",
        "    else:\n",
        "      res[i] = 2\n",
        "  #print \"Num correct: %d\" % np.sum(res == 0)\n",
        "  #print \"Num misclassified: %d\" % np.sum(res == 1)\n",
        "  #print \"Num others: %d\" % np.sum(res == 2)\n",
        "  return res\n",
        "\n",
        "def get_relative_estimator(label):\n",
        "  predictions = get_relative_predictions(label)\n",
        "  #print \"Creating decision tree for label:%d\" % label\n",
        "  estimator = tree.DecisionTreeClassifier()\n",
        "  estimator.fit(train_suffixes, predictions)\n",
        "  return estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHJlFvwXzXgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SLOW; run only if you want to build relative estimators.\n",
        "#relative_estimators = [None for _ in range(10)]\n",
        "#for i in range(10):\n",
        "#  relative_estimators[i] = get_relative_estimator(i)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBK9KWLc6TSe",
        "colab_type": "code",
        "outputId": "bf38ea78-9563-4ace-e552-26e3b0e45c2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/safednn-nasa/prophecy_DNN/master/marabou1.elf -O ./marabou1.elf\n",
        "!wget https://raw.githubusercontent.com/safednn-nasa/prophecy_DNN/master/MNIST_ITR_REL/mnist_10_layer.nnet -O ./mnist_10_layer.nnet\n",
        "!pwd\n",
        "!ls -lt ./marabou1.elf\n",
        "!chmod 777 ./marabou1.elf\n",
        "!ls -lt ./marabou1.elf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-07 08:36:27--  https://raw.githubusercontent.com/safednn-nasa/prophecy_DNN/master/marabou1.elf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4231816 (4.0M) [application/octet-stream]\n",
            "Saving to: ‘./marabou1.elf’\n",
            "\n",
            "./marabou1.elf      100%[===================>]   4.04M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2019-12-07 08:36:27 (66.9 MB/s) - ‘./marabou1.elf’ saved [4231816/4231816]\n",
            "\n",
            "--2019-12-07 08:36:28--  https://raw.githubusercontent.com/safednn-nasa/prophecy_DNN/master/MNIST_ITR_REL/mnist_10_layer.nnet\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 159523 (156K) [text/plain]\n",
            "Saving to: ‘./mnist_10_layer.nnet’\n",
            "\n",
            "./mnist_10_layer.nn 100%[===================>] 155.78K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-12-07 08:36:29 (7.00 MB/s) - ‘./mnist_10_layer.nnet’ saved [159523/159523]\n",
            "\n",
            "/content\n",
            "-rw-r--r-- 1 root root 4231816 Dec  7 08:36 ./marabou1.elf\n",
            "-rwxrwxrwx 1 root root 4231816 Dec  7 08:36 ./marabou1.elf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy0b_gz46_55",
        "colab_type": "text"
      },
      "source": [
        "## INVOKE MARABOU FOR VERIFYING POTENTIAL PROPERTIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLuPLXFC6_Rd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def invoke_marabou_chk(layer,neurons,signature,label):\n",
        "  #layer = 1\n",
        "  #neurons = [4, 8, 7, 1, 0, 2, 5, 3, 9, 6] \n",
        "  #signature = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "  #label = 6\n",
        "\n",
        "  for lab_indx in range(0,10):\n",
        "    if (lab_indx == label):\n",
        "      continue\n",
        "    strInp = \"\"\n",
        "    for i in range(0,784):\n",
        "      strInp = strInp + \"x\"+ str(i) + \" >= 0.0\" + \"\\n\"\n",
        "      #strInp = strInp + \"x\"+ str(i) + \" <= 1.0\" + \"\\n\"\n",
        "    #print(strInp)\n",
        "\n",
        "    strInternal = \"\"\n",
        "    for i in range(0,len(neurons)):\n",
        "      strInternal = strInternal + \"ws_\"+ str(layer) + \"_\" + str(neurons[i])\n",
        "      if (signature[i] == 0):\n",
        "         strInternal = strInternal + \" <= 0.0\" + \"\\n\"\n",
        "      else:\n",
        "         strInternal = strInternal + \" >= 0.0\"  + \"\\n\"\n",
        "\n",
        "    strOP = \"-y\"+ str(lab_indx) + \" +y\" + str(label) + \" <= 0.00\" + \"\\n\"\n",
        "\n",
        "    #Write to a property file\n",
        "    file1 = open('property.txt',\"w\")\n",
        "    #file1.writelines(strInp) \n",
        "    file1.writelines(strInternal) \n",
        "    file1.writelines(strOP) \n",
        "    file1.close() \n",
        "\n",
        "    file1 = open('property.txt',\"r\")  \n",
        "    print(\"PROPERTY FILE IS \")\n",
        "    print(file1.read())\n",
        "    file1.close()\n",
        "\n",
        "    !./marabou1.elf ./mnist_10_layer.nnet ./property.txt --summary-file=summary1.txt\n",
        "    print(\"SUMMARY:\")\n",
        "    f = open('summary1.txt', 'r')\n",
        "    file_contents = f.read()\n",
        "    print (file_contents)\n",
        "    f.close()\n",
        "    if (file_contents.find('UNSAT') == -1):\n",
        "        break\n",
        "  #f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK1Pr1agnsoi",
        "colab_type": "text"
      },
      "source": [
        "### Examine properties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbOQkQU0zBz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_decision_path(estimator, inp):\n",
        "  # Extract the decision path taken by an input as an ordered list of indices\n",
        "  # of the neurons that were evaluated.\n",
        "  # See: http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
        "  n_nodes = estimator.tree_.node_count\n",
        "  feature = estimator.tree_.feature\n",
        "\n",
        "  # First let's retrieve the decision path of each sample. The decision_path\n",
        "  # method allows to retrieve the node indicator functions. A non zero element of\n",
        "  # indicator matrix at the position (i, j) indicates that the sample i goes\n",
        "  # through the node j.\n",
        "  X_test = [inp]\n",
        "  node_indicator = estimator.decision_path(X_test)\n",
        "  # Similarly, we can also have the leaves ids reached by each sample.\n",
        "  leaf_id = estimator.apply(X_test)\n",
        "  # Now, it's possible to get the tests that were used to predict a sample or\n",
        "  # a group of samples. First, let's make it for the sample.\n",
        "  node_index = node_indicator.indices[node_indicator.indptr[0]:\n",
        "                                      node_indicator.indptr[1]]\n",
        "  neuron_ids = []\n",
        "  for node_id in node_index:\n",
        "    if leaf_id[0] == node_id:\n",
        "        continue\n",
        "    neuron_ids.append(feature[node_id])\n",
        "  return neuron_ids\n",
        "\n",
        "def get_suffix_cluster(neuron_ids, neuron_sig,suffixes=train_suffixes):\n",
        "  # Get the cluster of inputs that such that all inputs in the cluster\n",
        "  # have provided on/off signature for the provided neurons.\n",
        "  #\n",
        "  # The returned cluster is an array of indices (into mnist.train.images).\n",
        "  return np.where((suffixes[:, neuron_ids] == neuron_sig).all(axis=1))[0]\n",
        "\n",
        "def is_consistent_cluster(cluster, predictions):\n",
        "  # Check if all inputs within the cluster have the same prediction.\n",
        "  # 'cluster' is an array of input ids.\n",
        "  pred = predictions[cluster[0]]\n",
        "  for i in cluster:\n",
        "    if predictions[i] != pred:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def is_misclassified(i):\n",
        "  return train_predictions[i] != mnist.train.labels[i]\n",
        "\n",
        "def visualize_conductances(img, label, neuron_ids, only_on=False):\n",
        "  # Visualize the conductances for the provided image.\n",
        "  # Args:\n",
        "  # - img: the provided mnist image\n",
        "  # - label: prediction label w.r.t. conductance must be computed\n",
        "  # - neuron_ids: list of neurons indices from the suffix tensor for which\n",
        "  #    conductances must be computed.\n",
        "  # - only_on: If True then conductance is computed only for those neurons\n",
        "  #    that are on for the given image. \n",
        "  vis = [mnist_to_pil_img(img)]\n",
        "  suffix = fingerprint_signature([img],curr_lay)\n",
        "  sumigc = 0.0\n",
        "  for i, id in enumerate(neuron_ids):\n",
        "    if only_on and suffix[i] != 1:\n",
        "      continue  \n",
        "    igc = conductance(img, label, neuron_id=id)\n",
        "    for indx in range(0,len(igc)):\n",
        "      print(indx,igc[indx]) ## a -ve gradient indicates the pixel value decreases the value of the neuron, making it go towards zero and negative\n",
        "      if ((suffix[i] == 0) and (igc[indx] > 0.0)): ## in a property if we want the o/p of a neuron to be zero, pixels which increase the neurons output should be given negative weightage\n",
        "        igc[indx] = -(igc[indx])\n",
        "\n",
        "    sumigc = sumigc + igc \n",
        "  \n",
        "  avgigc = sumigc / len(neuron_ids) ## gradient of each pixel w.r.t entire property - HIGHER VALUE INDICATES THE PIXEL HAS HIGHER CHANCE OF MAINTAINING THE PROPERTY\n",
        "  maxval = abs(max(avgigc, key=abs))\n",
        "  minval = abs(min(avgigc, key=abs))\n",
        "  threshold = (maxval - minval)/2.0\n",
        "  print(\"MAX ATR:\", maxval, \"MIN ATR:\", minval, \"THRESH:\", threshold)\n",
        "  avgigc = 1.0 * avgigc * (abs(avgigc) >= threshold) ## pixels with less significance in either SAT or DIS-SAT the property get blacked out\n",
        "  \n",
        "  \n",
        "  vis.append(visualize_attrs2(255*mnist_to_rgb(img), mnist_to_rgb(avgigc)))\n",
        "  return combine(vis)\n",
        "\n",
        "def get_invariant_inp(estimator, ref_id, suffixes):\n",
        "  # Returns an invariant (property) found w.r.t. the provided reference input\n",
        "  # Args\n",
        "  #  - inp: reference input, shape <784,>\n",
        "  # Returns:\n",
        "  #  - cluster: Indices of training inputs that satisfy the invariant\n",
        "  #  - neuron_id: A list of neurons such that all inputs that agree with\n",
        "  #    the reference input on the on/off status of these neurons have the\n",
        "  #    same prediction as the reference input.\n",
        "  ref_img = mnist_inp_images[ref_id]\n",
        "  ref_suffix = suffixes[ref_id]\n",
        "  print('PREFIX',ref_suffix)\n",
        "  neuron_ids = get_decision_path(estimator, ref_suffix)\n",
        "  print('NEURON IDS',neuron_ids)\n",
        "  neuron_sig = ref_suffix[neuron_ids]\n",
        "  print('NEURON SIGNATURE',neuron_sig)\n",
        "  cluster = get_suffix_cluster(neuron_ids, neuron_sig,suffixes)\n",
        "  imgs = []\n",
        "  cnt = 0\n",
        "  for indx1 in range(0,len(cluster)):\n",
        "    img = mnist.train.images(cluster[indx1])\n",
        "    fnd = 1\n",
        "    for i in range(0,len(img)):\n",
        "      if (ref_img[i] != img[i]):\n",
        "        fnd = 0\n",
        "        break\n",
        "    if (fnd == 1):\n",
        "        ref_id = cnt\n",
        "    cnt = cnt + 1\n",
        "    imgs.append(img)\n",
        "    \n",
        "  imgs_suffixes = fingerprint_signature(imgs,t_fc2)\n",
        "  ref_suffix = imgs_suffixes[ref_id]\n",
        "  print('PREFIX',ref_suffix)\n",
        "  neuron_ids = get_decision_path(estimator, ref_suffix)\n",
        "  print('NEURON IDS',neuron_ids)\n",
        "  neuron_sig = ref_suffix[neuron_ids]\n",
        "  print('NEURON SIGNATURE',neuron_sig)\n",
        "  cluster = get_suffix_cluster(neuron_ids, neuron_sig,imgs_suffixes)\n",
        "    \n",
        "  return cluster, neuron_ids, neuron_sig\n",
        "\n",
        "def get_invariant(estimator, ref_id):\n",
        "  # Returns an invariant found w.r.t. the provided reference input\n",
        "  # Args\n",
        "  #  - ref_id: Index (into mnist.train.images) of the reference input\n",
        "  # Returns:\n",
        "  #  - cluster: Indices of training inputs that satisfy the invariant\n",
        "  #  - neuron_id: A list of neurons such that all inputs that agree with\n",
        "  #    the reference input on the on/off status of these neurons have the\n",
        "  #    same prediction as the reference input.\n",
        "  ref_img = mnist.train.images[ref_id]\n",
        "  ref_suffix = train_suffixes[ref_id]\n",
        "  neuron_ids = get_decision_path(estimator, ref_suffix)\n",
        "  neuron_sig = ref_suffix[neuron_ids]\n",
        "  cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "  return cluster, neuron_ids, neuron_sig\n",
        "\n",
        "\n",
        "def get_all_invariants(estimator):\n",
        "  # Returns a dictionary mapping each decision tree prediction class\n",
        "  # to a list of invariants. Each invariant is specified as a triple:\n",
        "  # - neuron ids\n",
        "  # - neuron signature (for the neuron ids)\n",
        "  # - number of training samples that hit it\n",
        "  # The neuron ids and neuron signature can be supplied to get_suffix_cluster\n",
        "  # to obtain the cluster of training instances that hit the invariant.\n",
        "  def is_leaf(node):\n",
        "    return estimator.tree_.children_left[node] == estimator.tree_.children_right[node]\n",
        "\n",
        "  def left_child(node):\n",
        "    return estimator.tree_.children_left[node]\n",
        "\n",
        "  def right_child(node):\n",
        "    return estimator.tree_.children_right[node]\n",
        "  \n",
        "  def get_all_paths_rec(node):\n",
        "    # Returns a list of triples corresponding to paths\n",
        "    # in the decision tree. Each triple consists of\n",
        "    # - neurons encountered along the path\n",
        "    # - signature along the path\n",
        "    # - prediction class at the leaf\n",
        "    # - number of training samples that hit the path\n",
        "    # The prediction class and number of training samples\n",
        "    # are set to -1 when the leaf is \"impure\".\n",
        "    feature = estimator.tree_.feature\n",
        "    if is_leaf(node):\n",
        "      values = estimator.tree_.value[node][0]\n",
        "      if len(np.where(values != 0)[0]) == 1:\n",
        "        cl = estimator.classes_[np.where(values != 0)[0][0]]\n",
        "        nsamples = estimator.tree_.n_node_samples[node]\n",
        "      else:\n",
        "        # impure node\n",
        "        cl = -1\n",
        "        nsamples = -1\n",
        "      return [[[], [], cl, nsamples]]\n",
        "    # If it is not a leaf both left and right childs must exist\n",
        "    paths = [[[feature[node]] + p[0], [0] + p[1], p[2], p[3]] for p in get_all_paths_rec(left_child(node))]\n",
        "    paths += [[[feature[node]] + p[0], [1] + p[1], p[2], p[3]] for p in get_all_paths_rec(right_child(node))]\n",
        "    return paths\n",
        "  paths =  get_all_paths_rec(0)\n",
        "  print(\"Obtained all paths\")\n",
        "  invariants = {}\n",
        "  for p in tqdm(paths):\n",
        "    neuron_ids, neuron_sig, cl, nsamples = p\n",
        "    if cl not in invariants:\n",
        "      invariants[cl] = []\n",
        "    # cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "    invariants[cl].append([neuron_ids, neuron_sig, nsamples])\n",
        "  for cl in invariants.keys():\n",
        "    invariants[cl] = sorted(invariants[cl], key=operator.itemgetter(2), reverse=True)\n",
        "  return invariants\n",
        "\n",
        "\n",
        "def describe_cluster(cluster, neuron_ids, show_samples=False):\n",
        "  neuron_sig = train_suffixes[cluster[0]][neuron_ids]\n",
        "  print(\"Num neurons in invariant\", len(neuron_ids))\n",
        "  print(\"Neuron id and signature\")\n",
        "  \n",
        "  for i in range(0,len(neuron_ids)):\n",
        "    print(\"id:\", neuron_ids[i], \"sig:\", neuron_sig[i])\n",
        "  \n",
        "  print(\"Cluster size: \", len(cluster))\n",
        "  print(\"Num misclassified\", len([i for i in cluster if is_misclassified(i)]))\n",
        "  if show_samples:\n",
        "    for i in range(10):\n",
        "      images = []\n",
        "      for j in range(10):\n",
        "        if 10*i + j >= len(cluster):\n",
        "          break\n",
        "        images.append(mnist_to_pil_img(mnist.train.images[cluster[10*i+j]]))\n",
        "      if len(images) > 0:\n",
        "        show_img(combine(images))\n",
        "  \n",
        "\n",
        "def describe_invariants_all_labels(all_invariants,prevlayer = t_fc1,layer = t_fc2,suffixes=train_suffixes,COMMON=False, DEC_PREFX= False):\n",
        "  \n",
        "  print(\"PRINTING PURE RULES WITH SUPPORT MORE THAN 50 FOR EVERY LABEL:\");\n",
        "  for cl, invs in all_invariants.items():\n",
        "    if (cl == -1):\n",
        "      continue\n",
        "    \n",
        "    for indx in range (0, len(invs)):\n",
        "      inv = invs[indx]\n",
        "      cls = get_suffix_cluster(inv[0],inv[1],suffixes)\n",
        "      \n",
        "      neurons = inv[0]\n",
        "      signature = inv[1]\n",
        "\n",
        "      if (len(cls) <= 50):\n",
        "        continue\n",
        "      print(\"Class:\", cl, \", Rule:(neurons:\",inv[0],\",signature:\",inv[1],\"), Support:\",inv[2],\", Num misclassified\", len([i for i in cls if is_misclassified(i)]));\n",
        "\n",
        "      print(\"PIXELS IMPACTING PROPERTY (conductance) for 10 inputs satisfy the property\")\n",
        "      interval = int(len(cls)/10)\n",
        "      print(\"INTERVAL:\", interval)\n",
        "      i = 0\n",
        "      while (i < len(cls)):\n",
        "        ref_id = cls[i]\n",
        "        if (is_misclassified(ref_id)):\n",
        "          print(\"MISCLASSIFIED\")\n",
        "        else:\n",
        "          print(\"CORRECTLY CLASSIFIED\")\n",
        "        show_img(visualize_conductances(mnist.train.images[ref_id], train_predictions[ref_id], inv[0], only_on=False))\n",
        "        i = i + interval\n",
        "\n",
        "      #invoke_marabou_chk(LAYER,neurons,signature,cl)\n",
        "\n",
        "      if (COMMON == True):\n",
        "          common_nodes(cls,suffixes)\n",
        "\n",
        "      if (DEC_PREFX == True):\n",
        "          decision_prefs(cls,suffixes)\n",
        "\n",
        "  return\n",
        "  \n",
        "def common_nodes(cls,suffixes):\n",
        "    cnt = 0\n",
        "    common = np.zeros(10,dtype=int)\n",
        "    prev = np.zeros(10,dtype=int)\n",
        "    \n",
        "    for indx in range(0, len(cls)):\n",
        "        i = cls[indx]\n",
        "        cnt = cnt + 1\n",
        "        for j in range(0,len(suffixes[i])):\n",
        "          if (common[j] == -1):\n",
        "             continue\n",
        "          if ((indx != 0) and (suffixes[i][j] != prev[j])):\n",
        "             common[j] = -1\n",
        "          else:\n",
        "             common[j] = suffixes[i][j]\n",
        "          prev[j] = suffixes[i][j]\n",
        "\n",
        "\n",
        "    print('COMMON NODES IN CLUSTER for CLASS:',cl,cnt)\n",
        "    com = []\n",
        "    for k in range(0,len(common)):\n",
        "        if (common[k] != -1):\n",
        "           com.append((k,common[k]))\n",
        "    print(com)\n",
        "\n",
        "    return\n",
        "    \n",
        "def decision_prefs(cls,suffixes):\n",
        "    images = mnist.train.images\n",
        "    imgsCom = []\n",
        "    imgs = []\n",
        "    for indx in range(0, len(cls)):\n",
        "        print('IMG:')\n",
        "        print(list(zip(images[cls[indx]])))\n",
        "        imgs.append(images[cls[indx]])\n",
        "        imgsCom.append(images[cls[indx]])\n",
        "            \n",
        "    dec_prefixes= fingerprint_signature(imgs,layer)\n",
        "    prefixes = []\n",
        "    for indx in range(0,len(dec_prefixes)):\n",
        "       dec_pref = dec_prefixes[indx]\n",
        "    \n",
        "       match = 0\n",
        "       for indx1 in range(0, len(prefixes)):\n",
        "          match = 1\n",
        "          for i in range(0,len(prefixes[indx1])):\n",
        "             if (dec_pref[i] != prefixes[indx1][i]):\n",
        "                match = 0\n",
        "                break\n",
        "          if (match == 1):\n",
        "             break\n",
        "    \n",
        "       if (match == 0):\n",
        "          prefixes.append(dec_pref)\n",
        "    \n",
        "    print('DECISION PREFIXES IN CLUSTER for CLASS:',cl,cnt)\n",
        "    for k in range(0,len(prefixes)):\n",
        "      print(prefixes[k])\n",
        "\n",
        "    return\n",
        "    \n",
        "  \n",
        "  #print('LAYER INPS:')\n",
        "  #min = np.zeros(10)\n",
        "  #max = np.zeros(10)\n",
        "  #for dim in range(0,10):\n",
        "  #    min[dim] = 1000\n",
        "  #    max[dim] = -1000\n",
        "          \n",
        "  #prevlayer_vals = get_prediction(imgsCom,prevlayer)      \n",
        "  #print('MIN, MAX LAYER INPS:',len(prevlayer_vals))\n",
        "  #for i in range(0,len(prevlayer_vals)):\n",
        "  #    if (i == 0):\n",
        "  #      print(zip(prevlayer_vals[i]))\n",
        "  #    for dim in range(0,10):\n",
        "  #        if ( prevlayer_vals[i][dim] < min[dim]):\n",
        "  #            min[dim] = prevlayer_vals[i][dim]\n",
        "  #        if ( prevlayer_vals[i][dim] > max[dim]):\n",
        "  #            max[dim] = prevlayer_vals[i][dim]\n",
        "    \n",
        "  #print('MIN')\n",
        "  #print(zip(min))\n",
        "  #print('MAX')\n",
        "  #print(zip(max))    \n",
        "    \n",
        "  #df = pd.DataFrame(df, columns=['Prediction Class', 'Num Instances', 'Num Invariants', 'Num Invariants with cluster size >= 10', 'Size of largest invariant cluster'])\n",
        "  #df = pd.DataFrame(df,columns=['Pred Class','Total #Neurons','# Invariants'])\n",
        "  #return df\n",
        "\n",
        "\n",
        "def describe_all_invariants(all_invariants):\n",
        "  df = []\n",
        "  for cl, invs in all_invariants.iteritems(): \n",
        "    inv = invs[0]\n",
        "    clus = get_suffix_cluster(inv[0],inv[1])\n",
        "    #print(len(clus))\n",
        "    misCl = 0\n",
        "    for i in range(0,len(clus)):\n",
        "      indx = clus[i]\n",
        "      if (is_misclassified(indx) == True):\n",
        "        misCl = misCl + 1\n",
        "    print('class:',cl,',masSup:',inv[2],',#misCl:',misCl)\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOUniL-T5-Sa",
        "colab_type": "code",
        "outputId": "6d77b57e-4eac-4909-c9ff-8cb70661cc02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "source": [
        "print(\"layer:\", LAYER)\n",
        "#\",label:\", LABEL)\n",
        "invariants = get_all_invariants(basic_estimator)\n",
        "describe_invariants_all_labels(invariants,prev_lay,curr_lay)\n",
        "\n",
        "\n",
        "#ref_id = 3\n",
        "#print(\"### Cluster ###\")\n",
        "#cluster, neuron_ids, neuron_sig = get_invariant(basic_estimator, ref_id)\n",
        "#describe_cluster(cluster, neuron_ids, show_samples=True)\n",
        "\n",
        "# Visualize  10 inputs in the cluster\n",
        "#for i in cluster[:10]:\n",
        "#show_img(visualize_conductances(mnist.train.images[ref_id], train_predictions[ref_id], neuron_ids, only_on=False))\n",
        "    \n",
        "#print \"###  BASIC DECISION TREE ###\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Print invariant stats\n",
        "#describe_all_invariants(invariants,t_fc1,t_fc1)\n",
        "#describe_all_invariantsFull(invariants,True,t_fc2)\n",
        "#print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "#print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "\n",
        "### OTHER INPUTS ####\n",
        "#for indxi in range(0,len(mnist_inp_images)):\n",
        "#    inp = mnist_inp_images[indxi]\n",
        "#    lab = mnist_inp_labels[indxi] \n",
        "#    print(indxi, lab)\n",
        "#    describe_input_INP(indxi)\n",
        "##    print \"###  BASIC DECISION TREE Cluster ###\"\n",
        "#    if (indxi == 0):\n",
        "#       cluster, neuron_ids, neuron_sig = get_invariant_inp(basic_estimator,indxi )\n",
        "#       print('LEN CLUSTER:',len(cluster),is_consistent_cluster(cluster,train_predictions))\n",
        " #  # describe_cluster(cluster, neuron_ids)\n",
        "\n",
        "#    print \"###  RELATIVE DECISION TREE Cluster ###\"\n",
        "#    cluster, neuron_ids, neuron_sig = get_invariant(relative_estimators[lab], indxi)\n",
        "#    describe_cluster(cluster, neuron_ids)\n",
        "   \n",
        "    \n",
        "#print \"###  BASIC DECISION TREE ###\"\n",
        "#invariants = get_all_invariants(basic_estimator)\n",
        "## Print invariant stats\n",
        "#describe_all_invariants1(invariants)\n",
        "#describe_all_invariantsFull(invariants,True,t_fc2)\n",
        "#print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "#print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "\n",
        "\n",
        "#for cl in invariants.keys():\n",
        "#  print(cl,invariants[cl])\n",
        "  \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 0\"\n",
        "#invariants = get_all_invariants(relative_estimators[0])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "\n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 1\"\n",
        "#invariants = get_all_invariants(relative_estimators[1])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "    \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 2\"\n",
        "#invariants = get_all_invariants(relative_estimators[2])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "    \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 3\"\n",
        "#invariants = get_all_invariants(relative_estimators[3])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "    \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 4\"\n",
        "#invariants = get_all_invariants(relative_estimators[4])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "    \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 5\"\n",
        "#invariants = get_all_invariants(relative_estimators[5])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "\n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 6\"\n",
        "#invariants = get_all_invariants(relative_estimators[6])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "    \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 7\"\n",
        "#invariants = get_all_invariants(relative_estimators[7])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "    \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 8\"\n",
        "#invariants = get_all_invariants(relative_estimators[8])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "#for cl in invariants.keys():\n",
        "#   print(cl,invariants[cl])\n",
        "    \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 9\"\n",
        "#invariants = get_all_invariants(relative_estimators[9])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 59/59 [00:00<00:00, 127954.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "layer: 7\n",
            "Obtained all paths\n",
            "PRINTING PURE RULES WITH SUPPORT MORE THAN 50 FOR EVERY LABEL:\n",
            "Class: 6 , Rule:(neurons: [9, 3, 2, 7, 1, 4] ,signature: [1, 1, 0, 1, 0, 1] ), Support: 4708 , Num misclassified 31\n",
            "PIXELS IMPACTING PROPERTY (conductance) for 10 inputs satisfy the property\n",
            "INTERVAL: 470\n",
            "CORRECTLY CLASSIFIED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-a3c386602abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#\",label:\", LABEL)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minvariants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_invariants\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdescribe_invariants_all_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvariants\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_lay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurr_lay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-33a28cea8102>\u001b[0m in \u001b[0;36mdescribe_invariants_all_labels\u001b[0;34m(all_invariants, prevlayer, layer, suffixes, COMMON, DEC_PREFX)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CORRECTLY CLASSIFIED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mshow_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisualize_conductances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'show_img' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isr62nPPlROL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Get all fine_grained_estimator invariants\n",
        "#fge_all_invariants = get_all_invariants(fine_grained_estimator)\n",
        "## Print invariant stats\n",
        "#df = describe_all_invariants(fge_all_invariants)\n",
        "#print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "#print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU7SHWilyZjc",
        "colab_type": "text"
      },
      "source": [
        "### Analyzing clusters of misclassified inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GPZ8nMyYCMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Examine the cluster for a misclasification (Groundtruth: 4, Prediction: 49)\n",
        "#invs = fge_all_invariants[49]\n",
        "#neuron_ids, neuron_sig, _ = invs[0]\n",
        "#cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "#describe_cluster(cluster, neuron_ids)\n",
        "\n",
        "## Visualize  10 inputs in the cluster\n",
        "#for i in cluster[:10]:\n",
        "#  describe_input(i)\n",
        " # # show_img(visualize_conductances(mnist.train.images[i], train_predictions[i], neuron_ids, only_on=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-yR4qbD44Qr",
        "colab_type": "text"
      },
      "source": [
        "### Test Accuracy Improvements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBZDfuH42u42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We use the fine_grained_estimator to check if an input belongs\n",
        "## to a pure cluster (i.e., prediction id of the form 10*label + label).\n",
        "## If so, we declare the network's prediction as a \"condident prediction\".\n",
        "## We measure the accuracy of confident_predictions.\n",
        "#fine_grained_estimator_test_predictions = fine_grained_estimator.predict(test_suffixes)\n",
        "#fine_grained_estimator_leaf_nodes = fine_grained_estimator.apply(test_suffixes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HT-Fqrp7Kt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_accuracy_for_label(label):\n",
        "  def get_confidence():\n",
        "    is_confident = (fine_grained_estimator_test_predictions == 10*label + label)\n",
        "    sufficient_samples = fine_grained_estimator.tree_.n_node_samples[fine_grained_estimator_leaf_nodes] >= 10\n",
        "    is_confident *= sufficient_samples\n",
        "    return is_confident\n",
        "  # Following are boolean array. For e.g., with_label[i] is True if\n",
        "  # image i has the given label\n",
        "  with_label = (mnist.test.labels == label)\n",
        "  is_correct = (test_predictions == mnist.test.labels)\n",
        "  with_label_and_correct = with_label*is_correct\n",
        "  is_confident = get_confidence()\n",
        "  with_label_and_correct_and_confident = with_label_and_correct*is_confident\n",
        "  with_label_and_confident = with_label*is_confident\n",
        "\n",
        "  total = np.sum(with_label)\n",
        "  num_conf = np.sum(with_label_and_confident) \n",
        "  num_correct = np.sum(with_label_and_correct)\n",
        "  num_correct_conf = np.sum(with_label_and_correct_and_confident)\n",
        "  return total, num_conf, num_correct, num_correct_conf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1ed8-tT82wW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df = []\n",
        "#grand_total = 0\n",
        "#grand_correct = 0\n",
        "#grand_conf = 0\n",
        "#grand_correct_conf = 0\n",
        "#for i in range(10):\n",
        "#  total, num_conf, num_correct, num_correct_conf = test_accuracy_for_label(i)\n",
        "#  grand_total += total\n",
        "#  grand_conf += num_conf\n",
        "#  grand_correct += num_correct\n",
        "#  grand_correct_conf += num_correct_conf\n",
        "#  acc = 1.0*num_correct/total\n",
        "#  conf_acc = 1.0*num_correct_conf/num_conf\n",
        "#  df.append([i, total, num_conf, acc, conf_acc])\n",
        "#df = pd.DataFrame(df, columns=['Label', 'Instances', 'ConfidentInstances',  'Acc', 'ConfidentAcc',])\n",
        "#display(df)\n",
        "#print \"Total Instances\", grand_total\n",
        "#print \"Num Confident Instances\", grand_conf\n",
        "#print \"Orig Accuracy\", 1.0*grand_correct/grand_total\n",
        "#print \"Confident Accuracy\", 1.0*grand_correct_conf/grand_conf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhKV-FYLv-Eb",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing the Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEZ8ZeuMo_9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!apt-get install graphviz\n",
        "#!pip install graphviz\n",
        "#import graphviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaOsPspkjuOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dot_data = tree.export_graphviz(basic_estimator, out_file=None) \n",
        "#graph = graphviz.Source(dot_data)  \n",
        "#graph "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}